{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "import preprocessing\n",
    "import validation\n",
    "import plotting\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "input_dir = '../../data/train/' # directory of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for use with AMD GPUs on Linux run: pip3 install torch --extra-index-url https://download.pytorch.org/whl/rocm5.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all scene names from the training data\n",
    "all_scenes = preprocessing.get_scenes(input_dir)\n",
    "all_scenes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all image pairs, along with their covisibility and fundamental matrix\n",
    "\n",
    "#scenes = all_scenes # load all scenes\n",
    "scenes = [all_scenes[6]] # load only the first scene\n",
    "\n",
    "pairs = preprocessing.load_pairs(scenes,input_dir)\n",
    "\n",
    "# Load either all pairs, or only withing a specific covisibility range\n",
    "#pairs = pairs.reset_index()\n",
    "#pairs = pairs.query('covisibility > 0.95').reset_index()\n",
    "pairs = pairs.query('0.7 < covisibility < 0.8').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if a GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LoFTR and load the outdoor weights\n",
    "matcher = KF.LoFTR(pretrained='outdoor')\n",
    "matcher = matcher.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LoFTR on the image pairs loaded previously. The output is a DataFrame containing all relevant data for each image pair analyzed.\n",
    "scene_list = []\n",
    "fund_matrix_list = []\n",
    "pair_list = []\n",
    "fund_matrix_eval = []\n",
    "pair_eval = []\n",
    "mkpts0_list = []\n",
    "mkpts1_list = []\n",
    "mconf_list = []\n",
    "\n",
    "for index, row in pairs.head(10).iterrows(): # Head controls how many image pairs are analyzed\n",
    "    \n",
    "    split_pair = pairs.pair[index].split('-')\n",
    "    img_id0 = split_pair[0]\n",
    "    img_id1 = split_pair[1]\n",
    "    \n",
    "    img0_pth = os.path.join(input_dir, pairs.scene[index], \"images\", str(img_id0 + '.jpg'))\n",
    "    img1_pth = os.path.join(input_dir, pairs.scene[index], \"images\", str(img_id1 + '.jpg'))\n",
    "    img0 = preprocessing.load_torch_image(img0_pth, device)\n",
    "    img1 = preprocessing.load_torch_image(img1_pth, device)\n",
    "    batch = {\"image0\": K.color.rgb_to_grayscale(img0), \n",
    "            \"image1\": K.color.rgb_to_grayscale(img1)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        matcher(batch)\n",
    "        mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "        mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "        mconf = batch['mconf'].cpu().numpy()\n",
    "        \n",
    "    F = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.2, 0.99999, 50000)\n",
    "    \n",
    "    scene_list.append(pairs.scene[index])\n",
    "    fund_matrix_list.append(F[0])\n",
    "    pair_list.append(pairs.pair[index])\n",
    "    fund_matrix_eval.append(\" \".join(str(num) for num in F[0].flatten().tolist()))\n",
    "    pair_eval.append(\";\".join([\"phototourism\",pairs.scene[index],pairs.pair[index]]))\n",
    "    mkpts0_list.append(mkpts0)\n",
    "    mkpts1_list.append(mkpts1)\n",
    "    mconf_list.append(mconf)\n",
    "    \n",
    "results = pd.DataFrame({'scene': scene_list, 'pair': pair_list, 'fund_matrix': fund_matrix_list, \n",
    "                        'mkpts0': mkpts0_list, 'mkpts1': mkpts1_list, 'mconf': mconf_list,\n",
    "                        'pair_eval': pair_eval, 'fund_matrix_eval': fund_matrix_eval}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the validation on all results and return the mean average accuracy\n",
    "maa = validation.evaluate(input_dir, results.pair_eval, results.fund_matrix_eval)\n",
    "print(f'mAA={maa} (n={len(results)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw matches for a single pair from the results\n",
    "\n",
    "index = 7 # select image pair from results DataFrame\n",
    "threshold = 0 # setting a confidence threshold between 0 and 1 (0 will plot all matches)\n",
    "\n",
    "df_draw = pd.DataFrame({'mkpts0': results.mkpts0[index].tolist(), 'mkpts1': results.mkpts1[index].tolist(), 'mconf': results.mconf[index].tolist()})\n",
    "\n",
    "img0_pth = os.path.join(input_dir, results.scene[index], \"images\", str(results.pair[index].split('-')[0] + '.jpg'))\n",
    "img1_pth = os.path.join(input_dir, results.scene[index], \"images\", str(results.pair[index].split('-')[1] + '.jpg'))\n",
    "img0 = preprocessing.load_image(img0_pth)\n",
    "img1 = preprocessing.load_image(img1_pth)\n",
    "\n",
    "color = cm.jet(df_draw.query(f'mconf > {threshold}').mconf)\n",
    "text = [\n",
    "    'LoFTR',\n",
    "    'Matches: {}'.format(len(df_draw.query(f'mconf > {threshold}')))]\n",
    "fig = plotting.make_matching_figure(img0, img1, np.array(df_draw.query(f'mconf > {threshold}').mkpts0.values.tolist()), \n",
    "                                    np.array(df_draw.query(f'mconf > {threshold}').mkpts1.values.tolist()), color, text=text, dpi=150, alpha = 0.1, lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Fundamental Matrix for a single image match and perform evaluation\n",
    "F = cv2.findFundamentalMat(np.array(df_draw.query(f'mconf > {threshold}').mkpts0.values.tolist()), np.array(df_draw.query(f'mconf > {threshold}').mkpts1.values.tolist()), cv2.USAC_MAGSAC, 0.2, 0.99999, 50000)\n",
    "single_evaluate_scene = results.scene[index]\n",
    "single_evaluate_pair = results.pair[index]\n",
    "single_evaluate_pair_eval = results.pair_eval[index]\n",
    "aa = validation.evaluate_single(input_dir, single_evaluate_pair_eval, \" \".join(str(num) for num in F[0].flatten().tolist()))\n",
    "print(f'Accuracy = {aa[0]}\\nAngle error (degrees) = {aa[1][single_evaluate_scene][single_evaluate_pair]}\\nDistance error (meters) = {aa[2][single_evaluate_scene][single_evaluate_pair]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting with Plotly instead of Matplotlib\n",
    "\n",
    "Plotly can also be used to plot the matches, however lines can't be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw matches for a single pair from the results with plotly (lines do not work)\n",
    "\n",
    "index = 0 # select image pair from results DataFrame\n",
    "threshold = 0 # setting a confidence threshold (0 == no threshold)\n",
    "\n",
    "df_draw = pd.DataFrame({'mkpts0': results.mkpts0[index].tolist(), 'mkpts1': results.mkpts1[index].tolist(), 'mconf': results.mconf[index].tolist()})\n",
    "\n",
    "img0_pth = os.path.join(input_dir, results.scene[index], \"images\", str(results.pair[index].split('-')[0] + '.jpg'))\n",
    "img1_pth = os.path.join(input_dir, results.scene[index], \"images\", str(results.pair[index].split('-')[1] + '.jpg'))\n",
    "img0 = preprocessing.load_image(img0_pth)\n",
    "img1 = preprocessing.load_image(img1_pth)\n",
    "\n",
    "color = cm.jet(df_draw.query(f'mconf > {threshold}').mconf)\n",
    "color_trans = color * np.full(color.shape,255)\n",
    "color_trans = np.delete(color_trans, -1, axis=1)\n",
    "color_plotly = [f'rgb({\",\".join(c)})' for c in color_trans.astype(str)]\n",
    "text = [\n",
    "    'LoFTR',\n",
    "    'Matches: {}'.format(len(df_draw.query(f'mconf > {threshold}')))]\n",
    "fig = plotting.make_matching_figure_plotly(img0, img1, np.array(df_draw.query(f'mconf > {threshold}').mkpts0.values.tolist()), \n",
    "                                    np.array(df_draw.query(f'mconf > {threshold}').mkpts1.values.tolist()), color_plotly, text=text, alpha = 0.1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier removal using DBSCAN\n",
    "\n",
    "We tried to improve LoFTR accuracy by removing outliers in the matched points with DBSCAN. However, this resulted in reduced in reduced accuracy for most image pairs, so we didn't include DBSCAN in our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take matched points for the left image and append the corresponding confidence values for each point\n",
    "X = results.mkpts0[index]\n",
    "X_conf = results.mconf[index]\n",
    "X_conf_reshaped = np.reshape(X_conf, (len(X), 1))\n",
    "X_concat = np.append(X, X_conf_reshaped, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling everything to values between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "X_concat_scaled = scaler.fit_transform(X_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the \"Elbow\" to find the optimal DBSCAN epsilon value. Usually 0.05-0.06 for our model.\n",
    "plt.figure(figsize=(10,5))\n",
    "nn = NearestNeighbors(n_neighbors=5).fit(X_concat_scaled)\n",
    "distances, idx = nn.kneighbors(X_concat_scaled)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing DBSCAN and plotting the resulting clusters\n",
    "clustering = DBSCAN(eps=0.05, min_samples=3).fit(X_concat_scaled)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(X_concat_scaled[:, 0], X_concat_scaled[:, 1], c = clustering.labels_, s = X_concat_scaled[:,2]*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the image match while excluding the points classified as outliers/noise\n",
    "df_draw['clustering_labels'] = clustering.labels_\n",
    "\n",
    "F = cv2.findFundamentalMat(np.array(df_draw.query(f'clustering_labels != -1').mkpts0.values.tolist()), np.array(df_draw.query(f'clustering_labels != -1').mkpts1.values.tolist()), cv2.USAC_MAGSAC, 0.2, 0.99999, 50000)\n",
    "single_evaluate_scene = results.scene[index]\n",
    "single_evaluate_pair = results.pair[index]\n",
    "single_evaluate_pair_eval = results.pair_eval[index]\n",
    "aa = validation.evaluate_single(input_dir, single_evaluate_pair_eval, \" \".join(str(num) for num in F[0].flatten().tolist()))\n",
    "print(f'Accuracy = {aa[0]}\\nAngle error (degrees) = {aa[1][single_evaluate_scene][single_evaluate_pair]}\\nDistance error (meters) = {aa[2][single_evaluate_scene][single_evaluate_pair]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da0107cc955c643a970c078967c47321515469ab36be1559d5e88ff85a30550a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
